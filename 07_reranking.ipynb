{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重新排序\n",
    "\n",
    "在 RAG（检索增强生成）的上下文中，检索结果的重新排序是一个关键步骤，它根据检索到的文档与输入查询的相关性对初始检索结果进行优化。此过程涉及使用更复杂的模型（例如交叉编码器）对检索到的文档进行重新评分，以更好地捕捉查询与文档之间的语义相似性。重新排序后的文档列表随后用作生成模型的输入，确保使用最相关和准确的信息来生成最终输出。\n",
    "\n",
    "![交叉编码器图像](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/CrossEncoder.png)\n",
    "\n",
    "了解更多信息请点击[这里](https://www.sbert.net/examples/applications/retrieve_rerank/README.html)\n",
    "\n",
    "以下是步骤：\n",
    "* [加载重新排序模型](#loading-the-reranking-model)\n",
    "* [加载检索结果](#loading-retrieval-results)\n",
    "* [计算重新排序得分](#calculating-the-re-ranking-scores)\n",
    "* [基于重新排序的文档生成回复](#using-merged-results-to-generate-a-reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化改进"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import Theme, ThemeManager\n",
    "import pathlib\n",
    "\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载重新排序模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder \n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "console.print(cross_encoder.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载检索结果\n",
    "\n",
    "我们将从之前的混合搜索笔记本中加载检索结果，以避免重复。我们可以忽略稠密索引和稀疏索引的得分，因为我们将基于文档/分块的文本计算重新排序得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "hybrid_search_results = {}\n",
    "with open('data/dense_results.json') as f:\n",
    "    dense_results = json.load(f)\n",
    "    for doc in dense_results:\n",
    "        hybrid_search_results[doc['id']] = doc\n",
    "with open('data/sparse_results.json') as f:\n",
    "    sparse_results = json.load(f)\n",
    "    for doc in sparse_results:\n",
    "        hybrid_search_results[doc['id']] = doc\n",
    "console.print(hybrid_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the query that we used for the retrieval of the above documents\n",
    "query = \"What is context size of Mixtral?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算重新排序得分\n",
    "\n",
    "我们使用 `cross_encoder` 来计算匹配得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [[query, doc['text']] for doc in hybrid_search_results.values()] \n",
    "scores = cross_encoder.predict(pairs) \n",
    "\n",
    "console.print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择前 3 个重新排序的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine scores with corresponding document IDs\n",
    "results_with_scores = [\n",
    "    (doc_id, hybrid_search_results[doc_id]['text'], score)\n",
    "    for doc_id, score in zip(hybrid_search_results.keys(), scores)\n",
    "]\n",
    "\n",
    "# Sort results by score in descending order and take the top 3\n",
    "top_results = sorted(results_with_scores, key=lambda x: x[2], reverse=True)[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rich.table import Table\n",
    "table = Table(title=\"Top 3 Documents after Reranking\", show_lines=True)\n",
    "\n",
    "table.add_column(\"ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Score\", justify=\"right\", style=\"green\", no_wrap=True)\n",
    "table.add_column(\"Document\", style=\"#e87d3e\")\n",
    "\n",
    "# Add rows to the table with top 3 results\n",
    "for doc_id, text, score in top_results:\n",
    "    table.add_row(str(doc_id), f\"{score:.4f}\", text)\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用合并结果生成回复\n",
    "\n",
    "我们现在可以获取改进后的合并结果，并调用 LLM 生成对用户查询的回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to hold the search results for the generation model\n",
    "search_results = [doc[1] for doc in top_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to connect to the large language model\n",
    "from openai import OpenAI\n",
    "from rich.text import Text\n",
    "\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are chatbot, an research expert. Your top priority is to help guide users to understand reserach papers.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": str(search_results)}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response_text = Text(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.panel import Panel\n",
    "\n",
    "panel = Panel(response_text, title=f\"Hybrid Search with Reranking Reply to \\\"{query}\\\"\")\n",
    "console.print(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
