{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混合搜索\n",
    "\n",
    "混合搜索将传统的基于关键词的搜索与语义搜索相结合，以提供更准确和相关的结果。在 RAG 应用中，它通过将基于关键词的搜索与语义搜索能力集成，促进了基于用户查询的相关研究文章的发现。这种集成使应用程序能够检索同时匹配关键词和语义的文章，使其特别适用于处理涉及微妙概念、同义词和相关想法的复杂查询。\n",
    "\n",
    "![混合搜索](images/Hybrid_Search.png)\n",
    "\n",
    "在本笔记本中，我们将深入探讨 RAG 应用中混合搜索方法的实现细节，探索它如何利用基于关键词和语义搜索技术来提供更有效的搜索体验。\n",
    "\n",
    "以下是步骤：\n",
    "* [加载分块数据集](#loading-the-chunks-from-the-previous-steps)\n",
    "* [稀疏索引](#Hybrid-Search---Sparse-Index)\n",
    "* [稠密索引](#hybrid-search---dense-index)\n",
    "* [合并结果](#hybrid-search---merging-results)\n",
    "* [使用合并结果生成回复](#using-merged-results-to-generate-a-reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化改进\n",
    "\n",
    "我们将使用 [rich 库](https://github.com/Textualize/rich) 来使输出更具可读性，并抑制警告信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import Theme, ThemeManager\n",
    "import pathlib\n",
    "\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合搜索 - 稀疏索引\n",
    "\n",
    "我们将使用支持 BM25 的数据库来补充向量数据库的语义搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "from bm25s.tokenization import Tokenizer, Tokenized\n",
    "import Stemmer  # optional: for stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载之前步骤中的分块\n",
    "\n",
    "我们将使用之前使用的 AI Arxiv 数据集中的分块。这些分块是通过语义分块切分并丰富了上下文的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "corpus_json = json.load(open('data/corpus.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建稀疏索引\n",
    "\n",
    "我们将使用基于 BM25 的内存索引。许多（向量）数据库原生支持 BM25，还有许多其他数据库支持对计算的稀疏向量进行索引和搜索。\n",
    "\n",
    "在此示例中，我们还将定义一个词干提取器和停用词，以清理文本并更好地选择将索引到稀疏索引中的标记/术语。\n",
    "\n",
    "分词器可以编码（将文本转换为 ID）和解码（将 ID 转换回文本）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "\n",
    "# optional: create a stemmer\n",
    "english_stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "# Initialize the Tokenizer with the stemmer\n",
    "sparse_tokenizer = Tokenizer(\n",
    "    stemmer=english_stemmer,\n",
    "    lower=True, # lowercase the tokens\n",
    "    stopwords=\"english\",  # or pass a list of stopwords\n",
    "    splitter=r\"\\w+\",  # by default r\"(?u)\\b\\w\\w+\\b\", can also be a function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(sparse_tokenizer.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
    "corpus_sparse_tokens = (\n",
    "    sparse_tokenizer\n",
    "    .tokenize(\n",
    "        corpus_text, \n",
    "        update_vocab=True, # update the vocab as we tokenize\n",
    "        return_as=\"ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the BM25 retriever and attach your corpus_json to it\n",
    "sparse_index = bm25s.BM25(corpus=corpus_json)\n",
    "# Now, index the corpus_tokens (the corpus_json is not used yet)\n",
    "sparse_index.index(corpus_sparse_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = sparse_tokenizer.get_vocab_dict()\n",
    "console.print(f\"The tokenizer vocabulary includes {len(vocab_dict)} tokens/terms\")\n",
    "\n",
    "focus_token = 'context'\n",
    "focus_token_index = vocab_dict.get(focus_token)\n",
    "console.print(f\"The index of the {focus_token} is {focus_token_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词器可以执行编码（将文本转换为 ID）和解码（将 ID 转换回文本）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(sparse_tokenizer.decode([[focus_token_index]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索稀疏索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(sparse_index.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每个标记，索引包含包含它的文档（分块）列表以及该标记在该文档（分块）中的得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.table import Table\n",
    "from rich.style import Style\n",
    "\n",
    "token_index = vocab_dict.get(focus_token)\n",
    "console.print(f\"Index of the token `{focus_token}` in the BM25 retriever: {token_index}\")\n",
    "score_index = sparse_index.scores.get('indptr')[token_index]\n",
    "next_score_index = sparse_index.scores.get('indptr')[token_index+1]\n",
    "\n",
    "table = Table(title=f\"Document Scores for `{focus_token}`\")\n",
    "\n",
    "table.add_column(\"Document ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Score\", justify=\"right\", style=\"bright_green\")\n",
    "\n",
    "max_score = max(sparse_index.scores['data'][score_index:next_score_index])\n",
    "# Define styles for specific rows\n",
    "highlight_style = Style(bgcolor=\"yellow\")\n",
    "\n",
    "for i in range(score_index, next_score_index):\n",
    "    doc_id = sparse_index.scores['indices'][i]\n",
    "    doc_score = sparse_index.scores['data'][i]\n",
    "    if doc_score == max_score:\n",
    "        table.add_row(\n",
    "            str(doc_id),\n",
    "            str(doc_score), style=highlight_style\n",
    "        )\n",
    "    else:\n",
    "        table.add_row(\n",
    "            str(doc_id),\n",
    "            str(doc_score)\n",
    "        )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搜索稀疏索引\n",
    "\n",
    "与在稠密索引中一样，我们需要对查询文本进行分词和编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the corpus\n",
    "query = \"What is context size of Mixtral?\"\n",
    "query_tokens = (\n",
    "    sparse_tokenizer\n",
    "    .tokenize(\n",
    "        [query], \n",
    "        update_vocab=False, \n",
    "        return_as=\"ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "console.print(query_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用编码后的查询来搜索稀疏索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the corpus\n",
    "sparse_results, sparse_scores = sparse_index.retrieve(query_tokens, k=10)\n",
    "\n",
    "for i in range(sparse_results.shape[1]):\n",
    "    doc, score = sparse_results[0, i], sparse_scores[0, i]\n",
    "    console.print(f\"Rank {i+1} (score: {score:.2f}): {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合搜索 - 稠密索引\n",
    "\n",
    "对于混合搜索，我们还需要使用向量数据库的稠密索引，正如我们在之前步骤中使用的那样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建稠密索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    \":memory:\"\n",
    ") \n",
    "\n",
    "# Create the embedding encoder\n",
    "dense_encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"hybrid_search\"\n",
    "\n",
    "dense_index = qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "        size=dense_encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "print(dense_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize!\n",
    "qdrant_client.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=dense_encoder.encode(doc[\"text\"]).tolist(),\n",
    "            payload=doc\n",
    "        ) for idx, doc in enumerate(corpus_json) # data is the variable holding all the enriched texts\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搜索稠密索引\n",
    "\n",
    "我们将首先使用稠密编码器对查询进行编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = dense_encoder.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用编码后的查询来搜索稠密索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_results = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(dense_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合搜索 - 合并结果\n",
    "\n",
    "有几种方法可以合并两种方法（稀疏和稠密）的结果。在本笔记本中，我们将使用简单的加权平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_scores = []\n",
    "for hit in dense_results:\n",
    "    doc_id = hit.payload[\"id\"]\n",
    "    doc_text = next((doc for doc in corpus_json if doc[\"id\"] == doc_id), None)[\"text\"]\n",
    "    doc_dense_score = hit.score\n",
    "    documents_with_scores.append({\n",
    "        \"id\": doc_id,\n",
    "        \"text\": doc_text,\n",
    "        \"dense_score\": doc_dense_score\n",
    "    })\n",
    "\n",
    "for i, result in enumerate(sparse_results[0]):\n",
    "    doc_id = result[\"id\"]\n",
    "    doc_text = next((doc for doc in corpus_json if doc[\"id\"] == doc_id), None)[\"text\"]\n",
    "    doc_sparse_score = sparse_scores[0][i]\n",
    "    for doc in documents_with_scores:\n",
    "        if doc[\"id\"] == doc_id:\n",
    "            doc[\"sparse_score\"] = doc_sparse_score\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(documents_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将对每个索引的得分进行归一化，然后计算一个加权得分，其中稠密索引的权重更高（0.8）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize the two types of scores\n",
    "dense_scores = np.array([doc.get(\"dense_score\", 0) for doc in documents_with_scores])\n",
    "sparse_scores = np.array([doc.get(\"sparse_score\", 0) for doc in documents_with_scores])\n",
    "\n",
    "dense_scores_normalized = (dense_scores - np.min(dense_scores)) / (np.max(dense_scores) - np.min(dense_scores))\n",
    "sparse_scores_normalized = (sparse_scores - np.min(sparse_scores)) / (np.max(sparse_scores) - np.min(sparse_scores))\n",
    "\n",
    "# Calculate a weighted score with alpha of 0.2 to the sparse score\n",
    "alpha = 0.2\n",
    "weighted_scores = (1 - alpha) * dense_scores_normalized + alpha * sparse_scores_normalized\n",
    "\n",
    "# Pick up the top 3 documents with the weighted score\n",
    "top_docs = sorted(\n",
    "    zip(\n",
    "        documents_with_scores, \n",
    "        weighted_scores\n",
    "    ), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用合并结果生成回复\n",
    "\n",
    "我们现在可以获取合并后的结果并调用 LLM 生成对用户查询的回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to hold the search results for the generation model\n",
    "search_results = [doc[0]['text'] for doc in top_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to connect to the large language model\n",
    "from openai import OpenAI\n",
    "from rich.text import Text\n",
    "\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are chatbot, an research expert. Your top priority is to help guide users to understand reserach papers.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": str(search_results)}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response_text = Text(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.panel import Panel\n",
    "\n",
    "panel = Panel(response_text, title=f\"Hybrid Search Reply to \\\"{query}\\\"\")\n",
    "console.print(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存检索到的文档，以便在下一个重新排序的笔记本中使用，该笔记本展示了更高级的混合搜索结果合并方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/dense_results.json', 'w') as f:\n",
    "    json.dump([dense_result.payload for dense_result in dense_results], f, default=str)\n",
    "\n",
    "with open('data/sparse_results.json', 'w') as f:\n",
    "    json.dump([sparse_result for sparse_result in sparse_results[0]], f, default=str)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
