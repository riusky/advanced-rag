{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过 Reverse HYDE 提升语义相似性\n",
    "\n",
    "我们想要检索的文档通常比用户的查询更长且格式不同。为了提高**基于用户查询的文档检索** (**R**etrieval) 的准确性，我们将从每个文档生成假设的潜在查询，并将它们用作文档的向量嵌入 - AKA Reverse Hyde。\n",
    "\n",
    "请注意，原始的 [Hyde 技术](https://arxiv.org/abs/2212.10496) 处理用户的输入查询，并从这些查询生成假设文档，然后使用这些假设文档来检索真实文档。而在 Reverse HYDE 中，处理是在索引文档时完成的，而不是在检索时。因此，查询的延迟不会受到影响。\n",
    "\n",
    "* [Reverse HYDE 实现](#reverse-hyde-implementation)\n",
    "* [用 Reverse HYDE 输出丰富向量数据库](#enriching-vector-database-with-reverse-hyde-output)\n",
    "* [查询增强后的索引](#query-the-enriched-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化改进\n",
    "\n",
    "我们将使用 [rich 库](https://github.com/Textualize/rich) 来使输出更具可读性，并抑制警告信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import Theme, ThemeManager\n",
    "import pathlib\n",
    "\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse HYDE 实现\n",
    "\n",
    "我们将创建一个类，用于生成假设问题并通过计算语义相似性匹配来检索文档。在实际应用中，我们可以使用向量数据库来存储、索引和检索嵌入向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseHyde:\n",
    "    def __init__(self, api_key: str):\n",
    "        openai.api_key = api_key\n",
    "        self.model = \"text-embedding-ada-002\"\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.embeddings.create(input=text, model=self.model)\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    def generate_reverse_hyde(self, chunk: str, n: int = 3) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "        \n",
    "Given the following text chunk, generate {n} different questions that this chunk would be a good answer to:\n",
    "\n",
    "Chunk: {chunk}\n",
    "\n",
    "Questions (enumarate the questions with 1. 2., etc.):\n",
    "\"\"\"\n",
    "\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        \n",
    "        questions = response.choices[0].message.content.strip().split('\\n')\n",
    "        return [q.split('. ', 1)[1] for q in questions if '. ' in q]\n",
    "\n",
    "    def process_chunks(self, chunks: List[str], n: int = 3) -> Dict[str, List[str]]:\n",
    "        processed_chunks = {}\n",
    "        for chunk in chunks:\n",
    "            processed_chunks[chunk] = self.generate_reverse_hyde(chunk, n)\n",
    "        return processed_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从环境变量加载 API 密钥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 Reverse HYDE 输出丰富向量数据库\n",
    "\n",
    "我们将在一组文档上应用 Reverse HYDE 方法，并用 LLM 生成的假设问题丰富向量数据库索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Usage example\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "reverse_hyde = ReverseHyde(api_key)\n",
    "\n",
    "chunks = [\n",
    "    \"A mitochondrion (pl. mitochondria) is an organelle found in the cells of most eukaryotes, such as animals, plants and fungi. Mitochondria have a double membrane structure and use aerobic respiration to generate adenosine triphosphate (ATP), which is used throughout the cell as a source of chemical energy. They were discovered by Albert von Kölliker in 1857 in the voluntary muscles of insects. Meaning a thread-like granule, the term mitochondrion was coined by Carl Benda in 1898. The mitochondrion is popularly nicknamed the \\\"powerhouse of the cell\\\", a phrase popularized by Philip Siekevitz in a 1957 Scientific American article of the same name.\",\n",
    "    \"Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\",\n",
    "    \"The American Civil War (from April 12, 1861 to May 26, 1865) was a civil war in the United States between the Union (\\\"the North\\\") and the Confederacy (\\\"the South\\\"), which was formed in 1861 by states that had seceded from the Union. The central conflict leading to war was a dispute over whether slavery should be permitted to expand into the western territories, leading to more slave states, or be prohibited from doing so, which many believed would place slavery on a course of ultimate extinction.\"\n",
    "]\n",
    "\n",
    "processed_chunks = reverse_hyde.process_chunks(chunks, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(processed_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询增强后的索引\n",
    "\n",
    "一旦我们拥有一个包含多个文档假设问题的索引，就可以用它来基于真实用户的查询检索文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What generates energy in a cell?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# create the vector database client\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
    "\n",
    "# Create the embedding encoder\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection to store the wine rating data\n",
    "hyde_collection_name=\"reverse_hyde\"\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=hyde_collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "# vectorize!\n",
    "qdrant.upload_points(\n",
    "    collection_name=hyde_collection_name,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid5(uuid.NAMESPACE_URL, f\"{d_idx}-{q_idx}\").hex,\n",
    "            vector=encoder.encode(question).tolist(),\n",
    "            payload={ \n",
    "                \"document\": document , \n",
    "                \"doc_id\": d_idx\n",
    "            }\n",
    "        ) for d_idx, (document, questions) \n",
    "            in enumerate(processed_chunks.items()) \n",
    "                for q_idx, question in enumerate(questions)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    qdrant\n",
    "    .get_collection(\n",
    "        collection_name=hyde_collection_name\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在集合中搜索最佳匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "\n",
    "def search_collection(collection_name: str, query: str, limit: int = 1):\n",
    "    \"\"\"\n",
    "    This function searches the specified collection for the best match to the given query.\n",
    "    It then creates a table and a panel to display the query and the best match.\n",
    "    \n",
    "    :param collection_name: The name of the collection to search.\n",
    "    :param query: The query to search for.\n",
    "    :param limit: The maximum number of results to return. Default is 1.\n",
    "    \"\"\"\n",
    "    hits = qdrant.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=encoder.encode(query).tolist(),\n",
    "        limit=limit\n",
    "    )\n",
    "    # Create a table for both query and best match\n",
    "    table = Table(show_header=True, header_style=\"bold yellow\")\n",
    "    table.add_column(\"Query\", style=\"bright_cyan\", width=30)\n",
    "    table.add_column(\"Best Matching Chunk\", style=\"bright_yellow\", width=50)\n",
    "    table.add_column(\"Score\", style=\"bright_green\")\n",
    "    for hit in hits:\n",
    "        table.add_row(query, f\"{hit.payload['document'][:80]}...\", \"{:.4f}\".format(hit.score))\n",
    "\n",
    "    # Create a panel for the table\n",
    "    panel = Panel(\n",
    "        table,\n",
    "        title=f\"[bold]Query and Best Match in {collection_name}\",\n",
    "        border_style=\"white\",\n",
    "        expand=False\n",
    "    )\n",
    "\n",
    "    # Print the panel\n",
    "    console.print(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_collection(hyde_collection_name, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与仅文档索引（无 HYDE）进行比较\n",
    "\n",
    "我们将对相同的文档进行索引，但不添加 Reverse HYDE 问题，并比较相似性得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection to store the wine rating data\n",
    "docs_collection_name=\"documents_only\"\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=docs_collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize!\n",
    "qdrant.upload_points(\n",
    "    collection_name=docs_collection_name,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=encoder.encode(document).tolist(),\n",
    "            payload={ \"document\": document}\n",
    "        ) for idx, (document, questions) in enumerate(processed_chunks.items())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_collection(docs_collection_name, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
