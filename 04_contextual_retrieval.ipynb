{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过上下文检索增强 RAG\n",
    "\n",
    "我们将使用 LLM 为每个分块和文档生成一个上下文句子，以提高其检索准确性并在混合搜索中使用。\n",
    "\n",
    "* [加载复杂文档数据集](#loading-a-complex-dataset-of-documents)\n",
    "* [将文档切分为分块](#split-the-documents-into-chunks)\n",
    "* [生成上下文句子](#generate-the-context-sentence)\n",
    "* [用上下文丰富分块嵌入向量](#enrich-the-chunk-embedding-vectors-with-the-context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化改进\n",
    "\n",
    "我们将使用 [rich 库](https://github.com/Textualize/rich) 来使输出更具可读性，并抑制警告信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import Theme, ThemeManager\n",
    "import pathlib\n",
    "\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载复杂文档数据集\n",
    "\n",
    "我们将加载一个来自 Arxiv 的复杂科学文档数据集。在这种文档上应用简单的分块方法在 RAG 应用中会导致较差的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "    features: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'summary'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'source'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'authors'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'categories'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'comment'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'journal_ref'</span>, \n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'primary_category'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'published'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'updated'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'content'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    num_rows: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2673</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    features: \u001b[1m[\u001b[0m\u001b[92m'id'\u001b[0m, \u001b[92m'title'\u001b[0m, \u001b[92m'summary'\u001b[0m, \u001b[92m'source'\u001b[0m, \u001b[92m'authors'\u001b[0m, \u001b[92m'categories'\u001b[0m, \u001b[92m'comment'\u001b[0m, \u001b[92m'journal_ref'\u001b[0m, \n",
       "\u001b[92m'primary_category'\u001b[0m, \u001b[92m'published'\u001b[0m, \u001b[92m'updated'\u001b[0m, \u001b[92m'content'\u001b[0m, \u001b[92m'references'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    num_rows: \u001b[91m2673\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jamescalam/ai-arxiv2\", split=\"train\")\n",
    "console.print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文档切分为分块\n",
    "\n",
    "我们将使用之前笔记本中使用的统计分块器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import StatisticalChunker\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "chunker = StatisticalChunker(\n",
    "    encoder=encoder,\n",
    "    min_split_tokens=100,\n",
    "    max_split_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_0 = chunker(docs=[dataset[\"content\"][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">╭──────────────────────────────────────────────────── Chunk 0 ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"font-weight: bold\">│</span> 4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang,         <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot,     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e.       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> experts). For every token, at each layer, a router network selects two experts to process the current state and <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> combine their outputs. Even though each token only sees two experts, the selected experts can be different at   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics,  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions,       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m╭─\u001b[0m\u001b[1m───────────────────────────────────────────────────\u001b[0m\u001b[1m Chunk 0 \u001b[0m\u001b[1m───────────────────────────────────────────────────\u001b[0m\u001b[1m─╮\u001b[0m\n",
       "\u001b[1m│\u001b[0m 4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang,         \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot,     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e.       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m experts). For every token, at each layer, a router network selects two experts to process the current state and \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m combine their outputs. Even though each token only sees two experts, the selected experts can be different at   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics,  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions,       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â                \u001b[1m│\u001b[0m\n",
       "\u001b[1m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.text import Text\n",
    "from rich.panel import Panel\n",
    "\n",
    "chunk_0_0 = ' '.join(chunks_0[0][0].splits)\n",
    "\n",
    "content = Text(chunk_0_0)\n",
    "console.print(Panel(content, title=f\"Chunk 0\", expand=False, border_style=\"bold\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成上下文句子\n",
    "\n",
    "我们将使用 Anthropic Claude 来生成上下文。它是最好的摘要生成 LLM 之一，并且引入了 [Prompt Caching](https://www.anthropic.com/news/prompt-caching)，这对于为同一文档的多个分块生成上下文非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "Here is the chunk we want to situate within the whole document\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "Answer only with the succinct context and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "def situate_context(doc: str, chunk: str) -> str:\n",
    "    response = client.beta.prompt_caching.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"} #we will make use of prompt caching for the full documents\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_context = situate_context(dataset[\"content\"][0], chunk_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">PromptCachingBetaMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'msg_01Lg8AiYwn7wHDFAsWGsjhGx'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">content</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">TextBlock</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">text</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'This chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Llama 2 70B and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">8x7B - Instruct model.'</span>,\n",
       "            <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">type</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">model</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'claude-3-haiku-20240307'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">role</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'assistant'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">stop_reason</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'end_turn'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">stop_sequence</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">type</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'message'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">usage</span>=<span style=\"color: #ffff00; text-decoration-color: #ffff00\">PromptCachingBetaUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">cache_creation_input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">cache_read_input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">12532</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">584</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">output_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">73</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93mPromptCachingBetaMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[92m'msg_01Lg8AiYwn7wHDFAsWGsjhGx'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[93mTextBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[1;38;2;232;125;62mtext\u001b[0m=\u001b[92m'This chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms \u001b[0m\n",
       "\u001b[92mLlama 2 70B and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral \u001b[0m\n",
       "\u001b[92m8x7B - Instruct model.'\u001b[0m,\n",
       "            \u001b[1;38;2;232;125;62mtype\u001b[0m=\u001b[92m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mmodel\u001b[0m=\u001b[92m'claude-3-haiku-20240307'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mrole\u001b[0m=\u001b[92m'assistant'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mstop_reason\u001b[0m=\u001b[92m'end_turn'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mstop_sequence\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mtype\u001b[0m=\u001b[92m'message'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62musage\u001b[0m=\u001b[93mPromptCachingBetaUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mcache_creation_input_tokens\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mcache_read_input_tokens\u001b[0m=\u001b[91m12532\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62minput_tokens\u001b[0m=\u001b[91m584\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62moutput_tokens\u001b[0m=\u001b[91m73\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(chunk_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_0_5 = ' '.join(chunks_0[0][5].splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_chunk_context = situate_context(dataset[\"content\"][0], chunk_0_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">PromptCachingBetaMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">id</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'msg_01Ri5m7g6sH1usc5WYs64td7'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">content</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">TextBlock</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">text</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'This chunk describes the architectural details of the Mixtral model, specifically the Sparse </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixture of Experts layer that is a key component of the model.'</span>,\n",
       "            <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">type</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">model</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'claude-3-haiku-20240307'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">role</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'assistant'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">stop_reason</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'end_turn'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">stop_sequence</span>=<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">None</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">type</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'message'</span>,\n",
       "    <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">usage</span>=<span style=\"color: #ffff00; text-decoration-color: #ffff00\">PromptCachingBetaUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">cache_creation_input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">cache_read_input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">12532</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">221</span>,\n",
       "        <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">output_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">35</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93mPromptCachingBetaMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1;38;2;232;125;62mid\u001b[0m=\u001b[92m'msg_01Ri5m7g6sH1usc5WYs64td7'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[93mTextBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[1;38;2;232;125;62mtext\u001b[0m=\u001b[92m'This chunk describes the architectural details of the Mixtral model, specifically the Sparse \u001b[0m\n",
       "\u001b[92mMixture of Experts layer that is a key component of the model.'\u001b[0m,\n",
       "            \u001b[1;38;2;232;125;62mtype\u001b[0m=\u001b[92m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mmodel\u001b[0m=\u001b[92m'claude-3-haiku-20240307'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mrole\u001b[0m=\u001b[92m'assistant'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mstop_reason\u001b[0m=\u001b[92m'end_turn'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mstop_sequence\u001b[0m=\u001b[2;37mNone\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62mtype\u001b[0m=\u001b[92m'message'\u001b[0m,\n",
       "    \u001b[1;38;2;232;125;62musage\u001b[0m=\u001b[93mPromptCachingBetaUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1;38;2;232;125;62mcache_creation_input_tokens\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62mcache_read_input_tokens\u001b[0m=\u001b[91m12532\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62minput_tokens\u001b[0m=\u001b[91m221\u001b[0m,\n",
       "        \u001b[1;38;2;232;125;62moutput_tokens\u001b[0m=\u001b[91m35\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(second_chunk_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用上下文丰富分块嵌入向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将生成的上下文与分块文本连接\n",
    "\n",
    "我们将遍历所有分块。根据分块的数量，这可能需要一些时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 46/46 [26:50<00:00, 35.00s/it] \n"
     ]
    }
   ],
   "source": [
    "arxiv_id = dataset[0][\"id\"]\n",
    "refs = list(dataset[0][\"references\"].values())\n",
    "doc_text = dataset[0][\"content\"]\n",
    "title = dataset[0][\"title\"]\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "corpus_json = []\n",
    "for i, chunk in tqdm(enumerate(chunks_0[0]), total=len(chunks_0[0]), desc=\"Processing chunks\"):\n",
    "    chunk_text = ' '.join(chunk.splits)\n",
    "    contextualized_text = situate_context(doc_text, chunk_text).content[0].text\n",
    "    corpus_json.append({\n",
    "        \"id\": i,\n",
    "        \"text\": f\"{chunk_text}\\n\\n{contextualized_text}\",\n",
    "        \"metadata\" : {\n",
    "            \"title\": title,\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"references\": refs\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'4 2 0 2 n a J 8 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> G L . s c </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> language model. Mixtral has the same </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i.e. experts</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">.</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">For every token, at each layer, a router network selects two experts to process the current state and combine their</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'chat model on human bench- marks. Both the base and instruct models are released under the Apache </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> with open weights, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">network chooses two of these groups </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">the â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">language model that outperforms Llama 2 70B and GPT-3.5 on most benchmarks. It describes the key architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details of Mixtral, including its use of a sparse mixture-of-experts network, and mentions that the base and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">instruct models are released under the Apache 2.0 license.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m0\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m G L . s c \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. \u001b[0m\n",
       "\u001b[92mJiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[92mDiego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio \u001b[0m\n",
       "\u001b[92mRenard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon \u001b[0m\n",
       "\u001b[92mAntoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed \u001b[0m\n",
       "\u001b[92mAbstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m language model. Mixtral has the same \u001b[0m\n",
       "\u001b[92marchitecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mi.e. experts\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m.\u001b[0m\n",
       "\u001b[92mFor every token, at each layer, a router network selects two experts to process the current state and combine their\u001b[0m\n",
       "\u001b[92moutputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a\u001b[0m\n",
       "\u001b[92mresult, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was \u001b[0m\n",
       "\u001b[92mtrained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all \u001b[0m\n",
       "\u001b[92mevaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and \u001b[0m\n",
       "\u001b[92mmultilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that \u001b[0m\n",
       "\u001b[92msurpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse \u001b[0m\n",
       "\u001b[92mmixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes\u001b[0m\n",
       "\u001b[92mthe model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'\u001b[0m,\n",
       "        \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m1\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'chat model on human bench- marks. Both the base and instruct models are released under the Apache \u001b[0m\n",
       "\u001b[92m2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # \u001b[0m\n",
       "\u001b[92mIntroduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m with open weights, \u001b[0m\n",
       "\u001b[92mlicensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset\u001b[0m\n",
       "\u001b[92mof its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput \u001b[0m\n",
       "\u001b[92mat large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the \u001b[0m\n",
       "\u001b[92mfeedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router \u001b[0m\n",
       "\u001b[92mnetwork chooses two of these groups \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mthe â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts \u001b[0m\n",
       "\u001b[92mlanguage model that outperforms Llama 2 70B and GPT-3.5 on most benchmarks. It describes the key architectural \u001b[0m\n",
       "\u001b[92mdetails of Mixtral, including its use of a sparse mixture-of-experts network, and mentions that the base and \u001b[0m\n",
       "\u001b[92minstruct models are released under the Apache 2.0 license.'\u001b[0m,\n",
       "        \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(corpus_json[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 corpus_json 保存到文件中\n",
    "\n",
    "我们希望在下一个笔记本中使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/corpus.json', 'w') as f:\n",
    "    json.dump(corpus_json, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
