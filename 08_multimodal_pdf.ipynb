{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于图像的文档索引与搜索（使用 ColPali 和 Qdrant）\n",
    "\n",
    "我们可以检索包含图像的文档，例如用户指南或旧扫描文档。我们将使用支持图像的嵌入模型来处理文档和查询。我们还将调整向量数据库以高效存储和搜索这些嵌入向量。\n",
    "\n",
    "以下是步骤：\n",
    "* [创建图像集合索引](#creating-image-collection-index)\n",
    "* [搜索图像索引](#searching-the-image-index)\n",
    "* [基于检索到的图像生成回复](#generate-response-with-the-retrieved-images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化改进"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import Theme, ThemeManager\n",
    "import pathlib\n",
    "\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建图像集合索引 <a id='creating-image-collection-index'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 PDF 文件转换为图像\n",
    "\n",
    "我们不希望依赖从 PDF 文件中提取文本，而是专注于页面的视觉内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "\n",
    "def convert_pdfs_to_images(pdf_folder):\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "    all_images = {}\n",
    "\n",
    "    for doc_id, pdf_file in enumerate(pdf_files):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        images = convert_from_path(pdf_path, poppler_path=r'/opt/homebrew/Cellar/poppler/24.04.0_1/bin')\n",
    "        all_images[pdf_file] = images\n",
    "\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_images = convert_pdfs_to_images(\"data/ikea/\")\n",
    "all_images = convert_pdfs_to_images(\"data/shokz/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(15, 10))\n",
    "\n",
    "first_pdf_key = next(iter(all_images))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = all_images[first_pdf_key][i]\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize ColPali model and processor\n",
    "model_name = (\n",
    "    \"vidore/colpali-v1.2\"  # Use the latest version available\n",
    ")\n",
    "colpali_model = ColPali.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n",
    ")\n",
    "colpali_processor = ColPaliProcessor.from_pretrained(\n",
    "    \"vidore/colpaligemma-3b-pt-448-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(colpali_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = all_images[first_pdf_key][0]\n",
    "with torch.no_grad():\n",
    "    sample_batch = colpali_processor.process_images([sample_image]).to(\n",
    "        colpali_model.device\n",
    "    )\n",
    "    sample_embedding = colpali_model(**sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.table import Table\n",
    "\n",
    "table = Table(title=\"Document Embedding\")\n",
    "table.add_column(\"Documents\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Tokens\", style=\"bright_yellow\")\n",
    "table.add_column(\"Vector Size\", style=\"green\")\n",
    "\n",
    "table.add_row(\n",
    "    str(sample_embedding.shape[0]), \n",
    "    str(sample_embedding.shape[1]), \n",
    "    str(sample_embedding.shape[2])\n",
    ")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    \":memory:\"\n",
    ")  # Use \":memory:\" for in-memory database or \"path/to/db\" for persistent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = sample_embedding.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "multi_vector_params = models.VectorParams(\n",
    "    size=vector_size,\n",
    "    distance=models.Distance.COSINE,\n",
    "    multivector_config=models.MultiVectorConfig(\n",
    "        comparator=models.MultiVectorComparator.MAX_SIM\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用量化减少向量内存占用\n",
    "\n",
    "我们可以定义一个 `ScalarQuantizationConfig` 并在创建集合时传递它。在服务器端，Qdrant 会将向量转换为 8 位整数，从而减少内存占用并加快搜索过程。您还可以切换 `always_ram` 参数，将向量保留在 RAM 中。这将提高性能，但会增加内存使用量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_quant = models.ScalarQuantizationConfig(\n",
    "    type=models.ScalarType.INT8,\n",
    "    quantile=0.99,\n",
    "    always_ram=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name=\"user-guides\"\n",
    "\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,  # the name of the collection\n",
    "    on_disk_payload=True,  # store the payload on disk\n",
    "    optimizers_config=models.OptimizersConfigDiff(\n",
    "        indexing_threshold=100\n",
    "    ),  # it can be useful to swith this off when doing a bulk upload and then manually trigger the indexing once the upload is done\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=vector_size,\n",
    "        distance=models.Distance.COSINE,\n",
    "        multivector_config=models.MultiVectorConfig(\n",
    "            comparator=models.MultiVectorComparator.MAX_SIM\n",
    "        ),\n",
    "        quantization_config=models.ScalarQuantization(\n",
    "            scalar=scalar_quant,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将编码后的图像插入向量数据库\n",
    "\n",
    "我们定义一个辅助函数，通过客户端将点上传到 Qdrant。我们使用 stamina 库来在网络问题的情况下启用重试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stamina\n",
    "\n",
    "@stamina.retry(on=Exception, attempts=3)\n",
    "def upsert_to_qdrant(batch):\n",
    "    try:\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points,\n",
    "            wait=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during upsert: {e}\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在将向量上传到 Qdrant。我们通过创建数据批次，将其传递给 ColPali 模型，然后将嵌入添加到 Qdrant 的 `PointStruct` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 2  # Adjust based on your GPU memory constraints\n",
    "\n",
    "total_images = sum(len(images) for images in all_images.values())\n",
    "\n",
    "# Use tqdm to create a progress bar\n",
    "with tqdm(total=total_images, desc=\"Indexing Progress\") as pbar:\n",
    "    for doc_id, pdf_file in enumerate(all_images.keys()):\n",
    "        for i in range(0, len(all_images[pdf_file]), batch_size):\n",
    "            images = all_images[pdf_file][i : i + batch_size]\n",
    "\n",
    "            # Process and encode images\n",
    "            with torch.no_grad():\n",
    "                batch_images = colpali_processor.process_images(images).to(\n",
    "                    colpali_model.device\n",
    "                )\n",
    "                image_embeddings = colpali_model(**batch_images)\n",
    "\n",
    "            # Prepare points for Qdrant\n",
    "            points = []\n",
    "            for j, embedding in enumerate(image_embeddings):\n",
    "                unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc_id}.{i + j}\"))\n",
    "                # Convert the embedding to a list of vectors\n",
    "                multivector = embedding.cpu().float().numpy().tolist()\n",
    "                points.append(\n",
    "                    models.PointStruct(\n",
    "                        id=unique_id,  \n",
    "                        vector=multivector,  # This is now a list of vectors\n",
    "                        payload={\n",
    "                            \"doc\": pdf_file, \n",
    "                            \"page\": i+j+1\n",
    "                        },  # can also add other metadata/data\n",
    "                    )\n",
    "                )\n",
    "            # Upload points to Qdrant\n",
    "            try:\n",
    "                upsert_to_qdrant(points)\n",
    "            # clown level error handling here 🤡\n",
    "            except Exception as e:\n",
    "                print(f\"Error during upsert: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "print(\"Indexing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果在上传期间关闭了索引，您可以通过设置较低的索引阈值来触发索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client.update_collection(\n",
    "    collection_name=collection_name,\n",
    "    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print( \n",
    "    qdrant_client\n",
    "    .get_collection(collection_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    qdrant_client\n",
    "    .scroll(\n",
    "        collection_name=collection_name, \n",
    "        limit=20\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搜索图像索引 <a id='searching-the-image-index'></a>\n",
    "\n",
    "一旦我们将编码后的图像上传到向量数据库，就可以对其进行查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_text = \"How do I answer a call?\"\n",
    "query_text = \"Why the led is flashing red and blue?\"\n",
    "with torch.no_grad():\n",
    "    batch_query = colpali_processor.process_queries([query_text]).to(\n",
    "        colpali_model.device\n",
    "    )\n",
    "    query_embedding = colpali_model(**batch_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(query_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the query embedding to a list of vectors\n",
    "multivector_query = query_embedding[0].cpu().float().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = qdrant_client.query_points(\n",
    "    collection_name=collection_name, \n",
    "    query=multivector_query, \n",
    "    limit=3, \n",
    "    timeout=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示搜索结果中的图像\n",
    "\n",
    "我们可以显示通过向量搜索检索到的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the top 3 images from the search result for display\n",
    "top_images = search_result.points[:6]\n",
    "\n",
    "# Create a figure with subplots for each image\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "# Iterate over the top images and plot each one\n",
    "for i, point in enumerate(top_images):\n",
    "    pdf_file = point.payload.get('doc')\n",
    "    page_num = int(point.payload.get('page')) - 1\n",
    "    img = all_images[pdf_file][page_num]\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title(f\"Score: {point.score}, \\n Doc: {pdf_file}\")\n",
    "    axs[i].axis('off')  # Do not display axes for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于检索到的图像生成回复  <a id='generate-response-with-the-retrieved-images'></a>\n",
    "\n",
    "在 **A**ugmentation（增强）步骤中，我们使用 base64 对检索到的图像进行编码，并将其作为提示的一部分与用户的查询一起发送给生成模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "top_image = search_result.points[0]\n",
    "pdf_file = top_image.payload.get('doc')\n",
    "page_num = int(top_image.payload.get('page')) - 1\n",
    "image = all_images[pdf_file][page_num]\n",
    "display(image)\n",
    "\n",
    "buffered = BytesIO()\n",
    "image.save(buffered, format=\"PNG\")  # You may choose another format if needed\n",
    "img_bytes = buffered.getvalue()\n",
    "\n",
    "image1_media_type = \"image/png\"\n",
    "\n",
    "image1_data = base64.standard_b64encode(img_bytes).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": image1_media_type,\n",
    "                        \"data\": image1_data,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": query_text\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "console.print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
