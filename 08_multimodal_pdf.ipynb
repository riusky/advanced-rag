{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸºäºå›¾åƒçš„æ–‡æ¡£ç´¢å¼•ä¸æœç´¢ï¼ˆä½¿ç”¨ ColPali å’Œ Qdrantï¼‰\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥æ£€ç´¢åŒ…å«å›¾åƒçš„æ–‡æ¡£ï¼Œä¾‹å¦‚ç”¨æˆ·æŒ‡å—æˆ–æ—§æ‰«ææ–‡æ¡£ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ”¯æŒå›¾åƒçš„åµŒå…¥æ¨¡å‹æ¥å¤„ç†æ–‡æ¡£å’ŒæŸ¥è¯¢ã€‚æˆ‘ä»¬è¿˜å°†è°ƒæ•´å‘é‡æ•°æ®åº“ä»¥é«˜æ•ˆå­˜å‚¨å’Œæœç´¢è¿™äº›åµŒå…¥å‘é‡ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯æ­¥éª¤ï¼š\n",
    "* [åˆ›å»ºå›¾åƒé›†åˆç´¢å¼•](#creating-image-collection-index)\n",
    "* [æœç´¢å›¾åƒç´¢å¼•](#searching-the-image-index)\n",
    "* [åŸºäºæ£€ç´¢åˆ°çš„å›¾åƒç”Ÿæˆå›å¤](#generate-response-with-the-retrieved-images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯è§†åŒ–æ”¹è¿›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import Theme, ThemeManager\n",
    "import pathlib\n",
    "\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ›å»ºå›¾åƒé›†åˆç´¢å¼• <a id='creating-image-collection-index'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°† PDF æ–‡ä»¶è½¬æ¢ä¸ºå›¾åƒ\n",
    "\n",
    "æˆ‘ä»¬ä¸å¸Œæœ›ä¾èµ–ä» PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼Œè€Œæ˜¯ä¸“æ³¨äºé¡µé¢çš„è§†è§‰å†…å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "\n",
    "def convert_pdfs_to_images(pdf_folder):\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "    all_images = {}\n",
    "\n",
    "    for doc_id, pdf_file in enumerate(pdf_files):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        images = convert_from_path(pdf_path, poppler_path=r'/opt/homebrew/Cellar/poppler/24.04.0_1/bin')\n",
    "        all_images[pdf_file] = images\n",
    "\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_images = convert_pdfs_to_images(\"data/ikea/\")\n",
    "all_images = convert_pdfs_to_images(\"data/shokz/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(15, 10))\n",
    "\n",
    "first_pdf_key = next(iter(all_images))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = all_images[first_pdf_key][i]\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize ColPali model and processor\n",
    "model_name = (\n",
    "    \"vidore/colpali-v1.2\"  # Use the latest version available\n",
    ")\n",
    "colpali_model = ColPali.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n",
    ")\n",
    "colpali_processor = ColPaliProcessor.from_pretrained(\n",
    "    \"vidore/colpaligemma-3b-pt-448-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(colpali_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = all_images[first_pdf_key][0]\n",
    "with torch.no_grad():\n",
    "    sample_batch = colpali_processor.process_images([sample_image]).to(\n",
    "        colpali_model.device\n",
    "    )\n",
    "    sample_embedding = colpali_model(**sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.table import Table\n",
    "\n",
    "table = Table(title=\"Document Embedding\")\n",
    "table.add_column(\"Documents\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Tokens\", style=\"bright_yellow\")\n",
    "table.add_column(\"Vector Size\", style=\"green\")\n",
    "\n",
    "table.add_row(\n",
    "    str(sample_embedding.shape[0]), \n",
    "    str(sample_embedding.shape[1]), \n",
    "    str(sample_embedding.shape[2])\n",
    ")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    \":memory:\"\n",
    ")  # Use \":memory:\" for in-memory database or \"path/to/db\" for persistent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = sample_embedding.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "multi_vector_params = models.VectorParams(\n",
    "    size=vector_size,\n",
    "    distance=models.Distance.COSINE,\n",
    "    multivector_config=models.MultiVectorConfig(\n",
    "        comparator=models.MultiVectorComparator.MAX_SIM\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨é‡åŒ–å‡å°‘å‘é‡å†…å­˜å ç”¨\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ª `ScalarQuantizationConfig` å¹¶åœ¨åˆ›å»ºé›†åˆæ—¶ä¼ é€’å®ƒã€‚åœ¨æœåŠ¡å™¨ç«¯ï¼ŒQdrant ä¼šå°†å‘é‡è½¬æ¢ä¸º 8 ä½æ•´æ•°ï¼Œä»è€Œå‡å°‘å†…å­˜å ç”¨å¹¶åŠ å¿«æœç´¢è¿‡ç¨‹ã€‚æ‚¨è¿˜å¯ä»¥åˆ‡æ¢ `always_ram` å‚æ•°ï¼Œå°†å‘é‡ä¿ç•™åœ¨ RAM ä¸­ã€‚è¿™å°†æé«˜æ€§èƒ½ï¼Œä½†ä¼šå¢åŠ å†…å­˜ä½¿ç”¨é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_quant = models.ScalarQuantizationConfig(\n",
    "    type=models.ScalarType.INT8,\n",
    "    quantile=0.99,\n",
    "    always_ram=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name=\"user-guides\"\n",
    "\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,  # the name of the collection\n",
    "    on_disk_payload=True,  # store the payload on disk\n",
    "    optimizers_config=models.OptimizersConfigDiff(\n",
    "        indexing_threshold=100\n",
    "    ),  # it can be useful to swith this off when doing a bulk upload and then manually trigger the indexing once the upload is done\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=vector_size,\n",
    "        distance=models.Distance.COSINE,\n",
    "        multivector_config=models.MultiVectorConfig(\n",
    "            comparator=models.MultiVectorComparator.MAX_SIM\n",
    "        ),\n",
    "        quantization_config=models.ScalarQuantization(\n",
    "            scalar=scalar_quant,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°†ç¼–ç åçš„å›¾åƒæ’å…¥å‘é‡æ•°æ®åº“\n",
    "\n",
    "æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œé€šè¿‡å®¢æˆ·ç«¯å°†ç‚¹ä¸Šä¼ åˆ° Qdrantã€‚æˆ‘ä»¬ä½¿ç”¨ stamina åº“æ¥åœ¨ç½‘ç»œé—®é¢˜çš„æƒ…å†µä¸‹å¯ç”¨é‡è¯•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stamina\n",
    "\n",
    "@stamina.retry(on=Exception, attempts=3)\n",
    "def upsert_to_qdrant(batch):\n",
    "    try:\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points,\n",
    "            wait=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during upsert: {e}\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ç°åœ¨å°†å‘é‡ä¸Šä¼ åˆ° Qdrantã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºæ•°æ®æ‰¹æ¬¡ï¼Œå°†å…¶ä¼ é€’ç»™ ColPali æ¨¡å‹ï¼Œç„¶åå°†åµŒå…¥æ·»åŠ åˆ° Qdrant çš„ `PointStruct` ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 2  # Adjust based on your GPU memory constraints\n",
    "\n",
    "total_images = sum(len(images) for images in all_images.values())\n",
    "\n",
    "# Use tqdm to create a progress bar\n",
    "with tqdm(total=total_images, desc=\"Indexing Progress\") as pbar:\n",
    "    for doc_id, pdf_file in enumerate(all_images.keys()):\n",
    "        for i in range(0, len(all_images[pdf_file]), batch_size):\n",
    "            images = all_images[pdf_file][i : i + batch_size]\n",
    "\n",
    "            # Process and encode images\n",
    "            with torch.no_grad():\n",
    "                batch_images = colpali_processor.process_images(images).to(\n",
    "                    colpali_model.device\n",
    "                )\n",
    "                image_embeddings = colpali_model(**batch_images)\n",
    "\n",
    "            # Prepare points for Qdrant\n",
    "            points = []\n",
    "            for j, embedding in enumerate(image_embeddings):\n",
    "                unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc_id}.{i + j}\"))\n",
    "                # Convert the embedding to a list of vectors\n",
    "                multivector = embedding.cpu().float().numpy().tolist()\n",
    "                points.append(\n",
    "                    models.PointStruct(\n",
    "                        id=unique_id,  \n",
    "                        vector=multivector,  # This is now a list of vectors\n",
    "                        payload={\n",
    "                            \"doc\": pdf_file, \n",
    "                            \"page\": i+j+1\n",
    "                        },  # can also add other metadata/data\n",
    "                    )\n",
    "                )\n",
    "            # Upload points to Qdrant\n",
    "            try:\n",
    "                upsert_to_qdrant(points)\n",
    "            # clown level error handling here ğŸ¤¡\n",
    "            except Exception as e:\n",
    "                print(f\"Error during upsert: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "print(\"Indexing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœåœ¨ä¸Šä¼ æœŸé—´å…³é—­äº†ç´¢å¼•ï¼Œæ‚¨å¯ä»¥é€šè¿‡è®¾ç½®è¾ƒä½çš„ç´¢å¼•é˜ˆå€¼æ¥è§¦å‘ç´¢å¼•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client.update_collection(\n",
    "    collection_name=collection_name,\n",
    "    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print( \n",
    "    qdrant_client\n",
    "    .get_collection(collection_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    qdrant_client\n",
    "    .scroll(\n",
    "        collection_name=collection_name, \n",
    "        limit=20\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æœç´¢å›¾åƒç´¢å¼• <a id='searching-the-image-index'></a>\n",
    "\n",
    "ä¸€æ—¦æˆ‘ä»¬å°†ç¼–ç åçš„å›¾åƒä¸Šä¼ åˆ°å‘é‡æ•°æ®åº“ï¼Œå°±å¯ä»¥å¯¹å…¶è¿›è¡ŒæŸ¥è¯¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_text = \"How do I answer a call?\"\n",
    "query_text = \"Why the led is flashing red and blue?\"\n",
    "with torch.no_grad():\n",
    "    batch_query = colpali_processor.process_queries([query_text]).to(\n",
    "        colpali_model.device\n",
    "    )\n",
    "    query_embedding = colpali_model(**batch_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(query_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the query embedding to a list of vectors\n",
    "multivector_query = query_embedding[0].cpu().float().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = qdrant_client.query_points(\n",
    "    collection_name=collection_name, \n",
    "    query=multivector_query, \n",
    "    limit=3, \n",
    "    timeout=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ˜¾ç¤ºæœç´¢ç»“æœä¸­çš„å›¾åƒ\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥æ˜¾ç¤ºé€šè¿‡å‘é‡æœç´¢æ£€ç´¢åˆ°çš„å›¾åƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the top 3 images from the search result for display\n",
    "top_images = search_result.points[:6]\n",
    "\n",
    "# Create a figure with subplots for each image\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "# Iterate over the top images and plot each one\n",
    "for i, point in enumerate(top_images):\n",
    "    pdf_file = point.payload.get('doc')\n",
    "    page_num = int(point.payload.get('page')) - 1\n",
    "    img = all_images[pdf_file][page_num]\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title(f\"Score: {point.score}, \\n Doc: {pdf_file}\")\n",
    "    axs[i].axis('off')  # Do not display axes for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºäºæ£€ç´¢åˆ°çš„å›¾åƒç”Ÿæˆå›å¤  <a id='generate-response-with-the-retrieved-images'></a>\n",
    "\n",
    "åœ¨ **A**ugmentationï¼ˆå¢å¼ºï¼‰æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ base64 å¯¹æ£€ç´¢åˆ°çš„å›¾åƒè¿›è¡Œç¼–ç ï¼Œå¹¶å°†å…¶ä½œä¸ºæç¤ºçš„ä¸€éƒ¨åˆ†ä¸ç”¨æˆ·çš„æŸ¥è¯¢ä¸€èµ·å‘é€ç»™ç”Ÿæˆæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "top_image = search_result.points[0]\n",
    "pdf_file = top_image.payload.get('doc')\n",
    "page_num = int(top_image.payload.get('page')) - 1\n",
    "image = all_images[pdf_file][page_num]\n",
    "display(image)\n",
    "\n",
    "buffered = BytesIO()\n",
    "image.save(buffered, format=\"PNG\")  # You may choose another format if needed\n",
    "img_bytes = buffered.getvalue()\n",
    "\n",
    "image1_media_type = \"image/png\"\n",
    "\n",
    "image1_data = base64.standard_b64encode(img_bytes).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": image1_media_type,\n",
    "                        \"data\": image1_data,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": query_text\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "console.print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
