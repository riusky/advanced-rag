{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 _sentence_transformer_ 的嵌入模型\n",
    "\n",
    "在本笔记本中，我们将探讨如何使用流行的 [sentence_transformers 库](https://sbert.net/index.html) 将包含多个单词/标记的文本编码为嵌入向量。我们将检查以下内容：\n",
    "\n",
    "* [OpenAI 嵌入](#openai-embedding)\n",
    "* [开源编码器的输入嵌入](#open-source-encoder---input-embeddings)\n",
    "* [开源编码器的输出嵌入（带上下文）](#open-source-encoder---output-embedding-with-context)\n",
    "* [针对查询和文档的改进编码器（双编码器）](#improved-encoder-for-queries-and-documents-bi-encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义丰富的主题以实现更好的对象打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import Theme, ThemeManager\n",
    "import pathlib\n",
    "\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=dark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI 嵌入 <a id='openai-embedding'></a>\n",
    "\n",
    "一个常见的选择是使用与生成模型相同的提供者提供的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"I have no interest in politics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 OpenAI 模块，用于与 OpenAI API 进行交互\n",
    "from openai import OpenAI\n",
    "\n",
    "# 创建一个 OpenAI 客户端实例，用于发送请求\n",
    "# client = OpenAI()\n",
    "client = OpenAI(api_key=\"sk-83db2355e64e4639ace2fbaaf75e1f4a\", base_url=\"https://api.deepseek.com/v1\")\n",
    "\n",
    "# 使用客户端创建文本嵌入（embedding），将输入的文本转换为向量表示\n",
    "response = client.embeddings.create(\n",
    "    # 输入的文本，通常是字符串形式\n",
    "    input=first_sentence,\n",
    "    # 指定使用的模型，这里使用的是 \"text-embedding-3-small\" 模型\n",
    "    # model=\"text-embedding-3-small\"  \n",
    "    model=\"embedding-2\"  \n",
    ")\n",
    "\n",
    "# 打印 API 返回的响应结果，通常包含嵌入向量等信息\n",
    "console.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开源编码器 - 输入嵌入\n",
    "\n",
    "我们将从 _sentence_transformers_ 库中的一个流行编码器开始。这将使我们能够探索其架构和流程，并在之后针对我们的用例进行优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 这是一个句子转换模型：它将句子和段落映射到384维的密集向量空间，并可用于像聚类或语义搜索这样的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的分词器\n",
    "\n",
    "我们将使用模型的默认分词器。每个单词或子词都会被转换为一个具有固定 ID 的标记。例如，在以下两个句子中，单词 `interest` 被分词为相同的 ID (`3037`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"I have no interest in politics\"\n",
    "second_sentence = \"The bank's interest rate rises\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_first_sentence = model.tokenize([first_sentence])\n",
    "console.rule(f\"{first_sentence}\")\n",
    "console.print(tokenized_first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_second_sentence = model.tokenize([second_sentence])\n",
    "console.rule(f\"{second_sentence}\")\n",
    "console.print(tokenized_second_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令牌ID可以用于将其转换回可读文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = (\n",
    "    model\n",
    "    .tokenizer\n",
    "    .convert_ids_to_tokens(\n",
    "        tokenized_second_sentence[\"input_ids\"]\n",
    "        [0]\n",
    "    )\n",
    ")\n",
    "\n",
    "console.print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = (\n",
    "    model\n",
    "    ._first_module()\n",
    "    .tokenizer\n",
    "    .get_vocab()\n",
    "    .items()\n",
    ")\n",
    "\n",
    "console.print(\"[bold]Vocabulary size[/bold]:\", len(vocabulary))\n",
    "console.print(dict(list(vocabulary)[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看一下分词器词汇表的一部分。我们将搜索 `interest` 的令牌，并查看它的邻近令牌。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sorted_vocabulary = sorted(\n",
    "    vocabulary, \n",
    "    key=lambda x: x[1],  # uses the value of the dictionary entry\n",
    ")\n",
    "sorted_tokens = [token for token, _ in sorted_vocabulary]\n",
    "\n",
    "focused_token = 'interest'\n",
    "# Find the index of the 'interest' token\n",
    "focused_index = sorted_tokens.index(focused_token)\n",
    "\n",
    "# Get 20 tokens around the focused token\n",
    "start_index = max(0, focused_index - 10)\n",
    "end_index = min(len(sorted_tokens), focused_index + 11)\n",
    "tokens_around_focused_index = sorted_tokens[start_index:end_index]\n",
    "\n",
    "from rich.table import Table\n",
    "\n",
    "table = Table(title=f\"Tokens around '{focused_token}':\")\n",
    "table.add_column(\"id\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"token\", style=\"bright_green\")\n",
    "\n",
    "for i, token in enumerate(tokens_around_focused_index, start=start_index):\n",
    "    if token == focused_token:\n",
    "        table.add_row(f\"[bold][black on yellow]{i}[/black on yellow][/bold]\", f\"[bold][black on yellow]{token}[/black on yellow][/bold]\")\n",
    "    else:\n",
    "        table.add_row(str(i), token)\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嵌入 Transformer 模型\n",
    "\n",
    "Transformer 由多个堆叠模块组成。标记是第一个模块的输入。让我们看看第一个模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_module = model._first_module()\n",
    "console.print(first_module.auto_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理（NLP）中，embeddings 是将词汇表中的令牌（tokens）映射到高维向量空间的过程。这些向量捕捉了词汇之间的语义和语法关系，是模型理解和生成文本的核心。让我们聚焦于 `embeddings` 部分:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = first_module.auto_model.embeddings\n",
    "console.print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嵌入模型的输入标记 ID\n",
    "\n",
    "我们将把上述两个句子发送到 Transformer 模型，并检查 **input** 标记之间的嵌入相似性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")  # Use MPS for Apple, CUDA for others, or fallback to CPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Tokenize both texts\n",
    "    first_tokens = model.tokenize([first_sentence])\n",
    "    second_tokens = model.tokenize([second_sentence])\n",
    "    \n",
    "    # Get the corresponding embeddings\n",
    "    first_embeddings = embeddings.word_embeddings(\n",
    "        first_tokens[\"input_ids\"].to(device)\n",
    "    )\n",
    "    second_embeddings = embeddings.word_embeddings(\n",
    "        second_tokens[\"input_ids\"].to(device)\n",
    "    )\n",
    "\n",
    "console.print(first_embeddings.shape, second_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.table import Table\n",
    "\n",
    "table = Table(title=\"Embeddings Shape Explanation\")\n",
    "\n",
    "table.add_column(\"Text\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Batch Size\", style=\"white\")\n",
    "table.add_column(\"Tokens Number\", style=\"white\")\n",
    "table.add_column(\"Embedding Dimension\", style=\"white\")\n",
    "\n",
    "table.add_row(\n",
    "    first_sentence,\n",
    "    str(first_embeddings.shape[0]),\n",
    "    str(first_embeddings.shape[1]),\n",
    "    str(first_embeddings.shape[2]),\n",
    ")\n",
    "table.add_row(\n",
    "    second_sentence,\n",
    "    str(second_embeddings.shape[0]),\n",
    "    str(second_embeddings.shape[1]),\n",
    "    str(second_embeddings.shape[2]),\n",
    ")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较令牌的输入嵌入\n",
    "\n",
    "在自然语言处理（NLP）中，输入嵌入 `（input embeddings）` 是将词汇表中的每个令牌 `（token）` 映射到一个高维向量空间的过程。这些嵌入向量捕捉了词汇的语义和语法信息，是模型理解和生成文本的基础。通过比较不同令牌的输入嵌入，我们可以了解它们在模型中的表示方式以及它们之间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate cosine similarity\n",
    "distances = util.cos_sim(\n",
    "    first_embeddings.squeeze(), \n",
    "    second_embeddings.squeeze()\n",
    ").cpu().numpy()\n",
    "\n",
    "# Get token labels\n",
    "x_labels = model.tokenizer.convert_ids_to_tokens(second_tokens[\"input_ids\"][0])\n",
    "y_labels = model.tokenizer.convert_ids_to_tokens(first_tokens[\"input_ids\"][0])\n",
    "\n",
    "# Create a DataFrame for Altair\n",
    "data = pd.DataFrame(\n",
    "    [(x, y, distances[i, j]) for i, y in enumerate(y_labels) for j, x in enumerate(x_labels)],\n",
    "    columns=['x', 'y', 'similarity']\n",
    ")\n",
    "\n",
    "# Create heatmap using Altair\n",
    "chart = alt.Chart(data).mark_rect().encode(\n",
    "    x=alt.X('x:O', title='Second Sentence Tokens', axis=alt.Axis(labelAngle=-45), sort=x_labels),\n",
    "    y=alt.Y('y:O', title='First Sentence Tokens', sort=y_labels),\n",
    "    color=alt.Color('similarity:Q', scale=alt.Scale(scheme='yellowgreenblue')),\n",
    "    tooltip=['x', 'y', alt.Tooltip('similarity:Q', format='.2f')]\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=400,\n",
    "    title='Input Token Similarity Heatmap'\n",
    ")\n",
    "\n",
    "# Add text labels\n",
    "text = chart.mark_text(baseline='middle').encode(\n",
    "    text=alt.Text('similarity:Q', format='.2f'),\n",
    "    color=alt.condition(\n",
    "        alt.datum.similarity > 0.5,\n",
    "        alt.value('white'),\n",
    "        alt.value('black')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine chart and text\n",
    "final_chart = (chart + text).configure_title(fontSize=16)\n",
    "\n",
    "# Display the chart\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词汇表嵌入\n",
    "\n",
    "正如我们所看到的，词汇表中有 30,522 个标记，每个标记都被嵌入为一个大小为 384 的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = first_module.auto_model \\\n",
    "    .embeddings \\\n",
    "    .word_embeddings \\\n",
    "    .weight \\\n",
    "    .detach() \\\n",
    "    .cpu() \\\n",
    "    .numpy()\n",
    "\n",
    "console.print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the embedding vectors to 2D for visualization\n",
    "\n",
    "We will use the TSNE library to create a 2D visualization of the token embeddings, to allow us to see tokens that are close to one another.\n",
    "\n",
    "This process can take about a minute or two, based on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, metric=\"cosine\", random_state=42)\n",
    "tsne_embeddings_2d = tsne.fit_transform(token_embeddings)\n",
    "console.print(tsne_embeddings_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标记嵌入可视化\n",
    "\n",
    "一旦我们将 384 维降维到 2D，就可以绘制它以进行探索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_colors = []\n",
    "for token in sorted_tokens:\n",
    "    if token[0] == \"[\" and token[-1] == \"]\": # Control Tokens\n",
    "        token_colors.append(\"red\")\n",
    "    elif token.startswith(\"##\"):            # Suffix Tokens\n",
    "        token_colors.append(\"blue\")\n",
    "    else:\n",
    "        token_colors.append(\"green\")        # All Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "# Enable VegaFusion data transformer to handle larger datasets\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame({\n",
    "    'x': tsne_embeddings_2d[:, 0],\n",
    "    'y': tsne_embeddings_2d[:, 1],\n",
    "    'token': sorted_tokens,\n",
    "    'color': token_colors\n",
    "})\n",
    "\n",
    "# Create the Altair chart\n",
    "chart = alt.Chart(df).mark_circle(size=30).encode(\n",
    "    x='x:Q',\n",
    "    y='y:Q',\n",
    "    color=alt.Color('color:N', scale=None),\n",
    "    tooltip=['token:N']\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=900,\n",
    "    title='Token Embeddings'\n",
    ").interactive()\n",
    "\n",
    "# Display the chart\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开源编码器 - 输出嵌入（带上下文）\n",
    "\n",
    "现在让我们看看 Transformer 嵌入模型输出端的标记嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embedding = model.encode([first_sentence])\n",
    "console.print(output_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_token_embeddings = model.encode(\n",
    "    [first_sentence], \n",
    "    output_value=\"token_embeddings\"\n",
    ")\n",
    "console.print(output_token_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    first_tokens = model.tokenize([first_sentence])\n",
    "    second_tokens = model.tokenize([second_sentence])\n",
    "    \n",
    "    first_output_embeddings = model.encode(\n",
    "        [first_sentence], \n",
    "        output_value=\"token_embeddings\"\n",
    "    )\n",
    "    second_output_embeddings = model.encode(\n",
    "        [second_sentence], \n",
    "        output_value=\"token_embeddings\"\n",
    "    )\n",
    "\n",
    "# Calculate cosine similarity\n",
    "distances = util.cos_sim(\n",
    "    first_output_embeddings[0], \n",
    "    second_output_embeddings[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化 **输出** 标记的相似性\n",
    "\n",
    "类似于我们从嵌入查找表中可视化 **输入** 标记相似性的方式，我们将在 Transformer 模型应用位置编码和注意力层后，可视化 **输出** 中相同标记的相似性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get token labels\n",
    "x_labels = model.tokenizer.convert_ids_to_tokens(second_tokens[\"input_ids\"][0])\n",
    "y_labels = model.tokenizer.convert_ids_to_tokens(first_tokens[\"input_ids\"][0])\n",
    "\n",
    "# Create a DataFrame for Altair\n",
    "data = pd.DataFrame(\n",
    "    [(x, y, distances[i, j]) for i, y in enumerate(y_labels) for j, x in enumerate(x_labels)],\n",
    "    columns=['x', 'y', 'similarity']\n",
    ")\n",
    "\n",
    "# Create heatmap using Altair\n",
    "chart = alt.Chart(data).mark_rect().encode(\n",
    "    x=alt.X('x:O', title='Second Sentence Tokens', axis=alt.Axis(labelAngle=-45), sort=x_labels),\n",
    "    y=alt.Y('y:O', title='First Sentence Tokens', sort=y_labels),\n",
    "    color=alt.Color('similarity:Q', scale=alt.Scale(scheme='yellowgreenblue', domain=[0, 1])),\n",
    "    tooltip=['x', 'y', alt.Tooltip('similarity:Q', format='.2f')]\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=400,\n",
    "    title='Output Token Similarity Heatmap'\n",
    ")\n",
    "\n",
    "# Add text labels\n",
    "text = chart.mark_text(baseline='middle').encode(\n",
    "    text=alt.Text('similarity:Q', format='.2f'),\n",
    "    color=alt.condition(\n",
    "        alt.datum.similarity > 0.5,\n",
    "        alt.value('white'),\n",
    "        alt.value('black')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine chart and text\n",
    "final_chart = (chart + text).configure_title(fontSize=16)\n",
    "\n",
    "# Display the chart\n",
    "final_chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine distance between output embeddings\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "\n",
    "def calculate_sentence_similarity(first_sentence, second_sentence):\n",
    "\n",
    "    first_embeddings = model.encode([first_sentence])\n",
    "    second_embeddings = model.encode([second_sentence])\n",
    "\n",
    "    # Reshape the embeddings to 2D arrays\n",
    "    first_embedding_2d = first_embeddings.reshape(1, -1)\n",
    "    second_embedding_2d = second_embeddings.reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine distance\n",
    "    cosine_distance = cosine_distances(first_embedding_2d, second_embedding_2d)[0][0]\n",
    "\n",
    "    # Note: Cosine distance is 1 - cosine similarity\n",
    "    cosine_similarity = 1 - cosine_distance\n",
    "\n",
    "    console.print(\n",
    "        Panel(\n",
    "            f\"[cyan bold]First Sentence:[/cyan bold] {first_sentence}\\n\"\n",
    "            f\"[cyan bold]Second Sentence:[/cyan bold] {second_sentence}\",\n",
    "            title=\"[green bold]Similarity Calculation[/green bold]\",\n",
    "            expand=False,\n",
    "            border_style=\"dim white\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = Table(title=\"Results\")\n",
    "    results.add_column(\"Metric\", style=\"bold\")\n",
    "    results.add_column(\"Value\", style=\"bold\")\n",
    "    results.add_row(\"Cosine Distance\", f\"{cosine_distance:.4f}\", style=\"cyan\")\n",
    "    results.add_row(\"Cosine Similarity\", f\"{cosine_similarity:.4f}\", style=\"bright_yellow\")\n",
    "\n",
    "    console.print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_sentence_similarity(first_sentence, second_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_sentence = \"Chase increased its lending fees\"\n",
    "\n",
    "calculate_sentence_similarity(second_sentence, third_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 针对查询和文档的改进编码器（bi-encoder）\n",
    "\n",
    "我们将使用 [上下文文档嵌入 (CDE)](https://huggingface.co/jxm/cde-small-v1)，这是 Hugging Face 模型库中的热门模型之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\MySpeace\\advanced-rag\\.venv\\Scripts\\python.exe\n",
      "transformers : 4.49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MySpeace\\advanced-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\riusk\\.cache\\huggingface\\hub\\models--answerdotai--ModernBERT-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabled 23 dropout modules from model type <class 'transformers_modules.jxm.cde-small-v2.287bf0ea6ebfecf2339762d0ef28fb846959a8f2.model.BiEncoder'>\n",
      "Disabled 46 dropout modules from model type <class 'transformers_modules.jxm.cde-small-v2.287bf0ea6ebfecf2339762d0ef28fb846959a8f2.model.ContextualDocumentEmbeddingTransformer'>\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import sys\n",
    "print(sys.executable)\n",
    "# improved_model = transformers.AutoModel.from_pretrained(\"jxm/cde-small-v2\", trust_remote_code=True)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"transformers :\", transformers.__version__)\n",
    "improved_model = transformers.AutoModel.from_pretrained(\"jxm/cde-small-v2\", trust_remote_code=True)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(improved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "corpus = load_dataset(\"BeIR/fiqa\", \"corpus\")[\"corpus\"]\n",
    "queries = load_dataset(\"BeIR/fiqa\", \"queries\")[\"queries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集样本\n",
    "\n",
    "让我们看看 [金融意见挖掘和问答 (FiQA) 数据集](https://huggingface.co/datasets/BeIR/fiqa) 的一些文档和查询示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "console.rule(\"Corpus Sample\")\n",
    "print(tabulate( \n",
    "    corpus\n",
    "    .to_pandas()\n",
    "    .head(10)\n",
    "    .assign(text_start=lambda x: x['text'].str[:100])\n",
    "    .drop(columns=['text','title'])\n",
    "    ,headers='keys', \n",
    "    tablefmt='github', \n",
    "    showindex=False\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.rule(\"Queries Sample\")\n",
    "print(tabulate( \n",
    "    queries\n",
    "    .to_pandas()\n",
    "    .head(10)\n",
    "    .assign(text_start=lambda x: x['text'].str[:100])\n",
    "    .drop(columns=['text','title'])\n",
    "    ,headers='keys', \n",
    "    tablefmt='github',\n",
    "    showindex=False\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一阶段：收集数据集嵌入\n",
    "\n",
    "CDE 的工作原理是首先从语料库文档中获取一组嵌入，这些嵌入旨在代表整个语料库。我们首先从语料库中采样一些文档（该模型使用每个上下文中的 512 个文档进行训练），并从我们的第一阶段模型中获取它们的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prefix = \"search_query: \"\n",
    "document_prefix = \"search_document: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_ex_document(ex: dict) -> dict:\n",
    "  ex[\"text\"] = f\"{ex['title']} {ex['text']}\"\n",
    "  return ex\n",
    "\n",
    "corpus_size = improved_model.config.transductive_corpus_size\n",
    "console.print(f\"Choosing {corpus_size} out of {len(corpus)} documents\")\n",
    "minicorpus_docs = corpus.select(random.choices(list(range(len(corpus))), k=corpus_size))\n",
    "minicorpus_docs = minicorpus_docs.map(process_ex_document)[\"text\"]\n",
    "minicorpus_docs = tokenizer(\n",
    "    [document_prefix + doc for doc in minicorpus_docs],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "minicorpus_docs = minicorpus_docs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset_embeddings = []\n",
    "for i in tqdm(range(0, len(minicorpus_docs[\"input_ids\"]), batch_size)):\n",
    "    minicorpus_docs_batch = {k: v[i:i+batch_size] for k,v in minicorpus_docs.items()}\n",
    "    with torch.no_grad():\n",
    "        dataset_embeddings.append(\n",
    "            improved_model.first_stage_model(**minicorpus_docs_batch)\n",
    "        )\n",
    "\n",
    "dataset_embeddings = torch.cat(dataset_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二阶段：在上下文中嵌入\n",
    "\n",
    "现在我们有了数据集嵌入，我们可以像平常一样使用它们来嵌入查询和文档。我们只需要提供一个额外的参数（CDE 代码中的 `dataset_embeddings`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = corpus.select(range(16)).map(process_ex_document)[\"text\"]\n",
    "\n",
    "docs_tokens = tokenizer(\n",
    "    [document_prefix + doc for doc in sample_docs],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  doc_embeddings = improved_model.second_stage_model(\n",
    "      input_ids=docs_tokens[\"input_ids\"],\n",
    "      attention_mask=docs_tokens[\"attention_mask\"],\n",
    "      dataset_embeddings=dataset_embeddings,\n",
    "  )\n",
    "doc_embeddings /= doc_embeddings.norm(p=2, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_sample = queries.select(range(16))[\"text\"]\n",
    "queries_tokens = tokenizer(\n",
    "    [query_prefix + query for query in queries_sample],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  query_embeddings = improved_model.second_stage_model(\n",
    "      input_ids=queries_tokens[\"input_ids\"],\n",
    "      attention_mask=queries_tokens[\"attention_mask\"],\n",
    "      dataset_embeddings=dataset_embeddings,\n",
    "  )\n",
    "query_embeddings /= query_embeddings.norm(p=2, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型比较\n",
    "\n",
    "让我们在文档和查询的样本上比较这两个模型（基础模型和带上下文的改进模型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  doc_basic_embeddings = model.encode(sample_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  queries_basic_embeddings = model.encode(queries_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Heatmap for improved model\n",
    "sns.heatmap((doc_embeddings @ query_embeddings.T).cpu(), cmap=\"jet\", ax=ax1, vmin=0, vmax=1)\n",
    "ax1.set_title(\"Improved Model\", fontsize=16)\n",
    "\n",
    "# Heatmap for basic model\n",
    "sns.heatmap((doc_basic_embeddings @ queries_basic_embeddings.T), cmap=\"jet\", ax=ax2 ,vmin=0, vmax=1)\n",
    "ax2.set_title(\"Basic Model\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "console.rule(\"Embedding Model Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
